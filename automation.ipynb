{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37551503-af0f-4189-80cd-335afdc0fe4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:99: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:188: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:273: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:344: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:429: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:513: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:586: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:675: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:754: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:825: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:915: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:99: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:188: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:273: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:344: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:429: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:513: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:586: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:675: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:754: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:825: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:915: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:99: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:188: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:273: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:344: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:429: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:513: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:586: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:675: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:754: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:825: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:915: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table updated successfully with the latest epl standings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  epl_schd_sql['League'] = 'EPL'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table updated successfully with the latest epl schd.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  epl_scorers_sql['league']='Epl'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table cleared successfully.\n",
      "Table updated successfully with the latest epl Top Scoreres.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:257: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laliga_standings_sql['league'] = 'Laliga'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:259: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  laliga_standings_sql.rename(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table updated successfully with the latest laliga standings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:335: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  lal_schd_sql['league'] = 'Laliga'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table updated successfully with the latest laliga schd.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:420: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  lal_scorers_sql['league']='Laliga'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table cleared successfully.\n",
      "Table updated successfully with the latest laliga top scorers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:499: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bundesliga_standings_sql.rename(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table updated successfully with the latest bundesliga standings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:577: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bun_schd_sql['league'] = 'bundesliga'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table updated successfully with the latest bundesliga schd.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bun_scorers_sql['league']='bundesliga'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table cleared successfully.\n",
      "Table updated successfully with the latest bundesliga top scorers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  seria_standings_sql['league'] = 'Laliga'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:740: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  seria_standings_sql.rename(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table updated successfully with the latest seria standings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  seria_schd_sql['league'] = 'Seria'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table updated successfully with the latest seria schd.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  seria_scorers_sql['league']='seria'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table cleared successfully.\n",
      "Table updated successfully with the latest seria Top Scorers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:99: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:188: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:273: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:344: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:429: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:513: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:586: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:675: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:754: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:825: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n",
      "C:\\Users\\Rohith R\\AppData\\Local\\Temp\\ipykernel_8328\\1104457689.py:915: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  server = 'Max\\SQLEXPRESS01'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 951\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    950\u001b[0m     update_football_tables()\n\u001b[1;32m--> 951\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def update_football_tables():\n",
    "   \n",
    "    # EPL STANDINGS\n",
    "    \n",
    "    point_table_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\"\n",
    "    response = requests.get(point_table_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    standings_table = soup.find('table', {'id': 'results2024-202591_overall'})\n",
    "    headers = [th.text for th in standings_table.find('thead').find_all('th')]\n",
    "    rows = standings_table.find('tbody').find_all('tr')\n",
    "    \n",
    "    # Prepare the DataFrame\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        data.append([cell.text.strip() for cell in cells])\n",
    "    \n",
    "    epl_standings = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    # Filter necessary columns\n",
    "    epl_standings_sql = epl_standings[['Rk', 'Squad', 'MP', 'W', 'D', 'L', 'Pts', 'Last 5', 'Top Team Scorer']]\n",
    "    \n",
    "    # Handle NaN values\n",
    "    epl_standings_sql = epl_standings_sql.where(pd.notnull(epl_standings_sql), None)\n",
    "    \n",
    "    # Connect to SQL Server\n",
    "    server = 'Max\\\\SQLEXPRESS01'\n",
    "    database = 'Football'\n",
    "    conn = pyodbc.connect(f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Upsert logic using MERGE\n",
    "    for index, row in epl_standings_sql.iterrows():\n",
    "        merge_query = \"\"\"\n",
    "        MERGE EPL_Standings AS target\n",
    "        USING (VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)) AS source (Rk, Squad, MP, W, D, L, Pts, Last5, TopTeamScorer)\n",
    "        ON target.Squad = source.Squad\n",
    "        WHEN MATCHED THEN \n",
    "            UPDATE SET \n",
    "                Rk = source.Rk, \n",
    "                MP = source.MP, \n",
    "                W = source.W, \n",
    "                D = source.D, \n",
    "                L = source.L, \n",
    "                Pts = source.Pts, \n",
    "                Last5 = source.Last5, \n",
    "                TopTeamScorer = source.TopTeamScorer\n",
    "        WHEN NOT MATCHED THEN \n",
    "            INSERT (Rk, Squad, MP, W, D, L, Pts, Last5, TopTeamScorer)\n",
    "            VALUES (source.Rk, source.Squad, source.MP, source.W, source.D, source.L, source.Pts, source.Last5, source.TopTeamScorer);\n",
    "        \"\"\"\n",
    "        cursor.execute(merge_query, row['Rk'], row['Squad'], row['MP'], row['W'], row['D'], row['L'], row['Pts'], row['Last 5'], row['Top Team Scorer'])\n",
    "    \n",
    "    # Commit the transaction and close the connection\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"Table updated successfully with the latest epl standings.\")\n",
    "\n",
    "\n",
    "    # EPL FIXTURES\n",
    "    \n",
    "    epl_fixture_url = \"https://fbref.com/en/comps/9/schedule/Premier-League-Scores-and-Fixtures\"\n",
    "    \n",
    "    response = requests.get(epl_fixture_url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    epl_sched_2024_2025 = soup.find('table',{'id': 'sched_2024-2025_9_1'})\n",
    "    \n",
    "    headers = [th.text for th in epl_sched_2024_2025.find('thead').find_all('th')]\n",
    "    \n",
    "    rows = epl_sched_2024_2025.find('tbody').find_all('tr')\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        # Extract all cells in each row\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        data.append([cell.text.strip() for cell in cells])\n",
    "    \n",
    "    epl_schd = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    epl_schd.head()\n",
    "\n",
    "    epl_schd_sql = epl_schd[['Wk','Day','Date','Time','Home','Score','Away','Venue']]\n",
    "    # Add a new column 'League' with value 'EPL'\n",
    "    epl_schd_sql['League'] = 'EPL'\n",
    "    \n",
    "    # Handle NaN values\n",
    "    epl_schd_sql = epl_schd_sql.where(pd.notnull(epl_schd_sql), None)\n",
    "    \n",
    "    # Connect to SQL Server\n",
    "    server = 'Max\\SQLEXPRESS01'\n",
    "    database = 'Football'\n",
    "    conn = pyodbc.connect(f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Upsert logic using MERGE\n",
    "    for index, row in epl_schd_sql.iterrows():\n",
    "        merge_query = \"\"\"\n",
    "        MERGE EPL_Schedule AS target\n",
    "        USING (VALUES (?, ?, ?, ?, ?, ?, ?,?)) AS source (Wk, Day, Date, Time, Home, Score, Away,Venue)\n",
    "        ON target.Wk = source.Wk and target.Date = source.Date and target.Home = source.Home and target.Away = source.Away\n",
    "        WHEN MATCHED THEN \n",
    "            UPDATE SET \n",
    "                Wk = source.Wk, \n",
    "                Day = source.Day, \n",
    "                Date = source.Date, \n",
    "                Time = source.Time, \n",
    "                Home = source.Home, \n",
    "                Score = source.Score, \n",
    "                Away = source.Away,\n",
    "                Venue = source.Venue;\n",
    "       \n",
    "        \"\"\"\n",
    "        cursor.execute(merge_query, row['Wk'], row['Day'], row['Date'], row['Time'], row['Home'], row['Score'], row['Away'],row['Venue'])\n",
    "    \n",
    "    # Commit the transaction and close the connection\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"Table updated successfully with the latest epl schd.\")\n",
    "\n",
    "\n",
    "    # EPL TOP SCORERES\n",
    "    \n",
    "    epl_scores_url = \"https://fbref.com/en/stathead/player_comparison.cgi?request=1&sum=0&comp_type=spec&dom_lg=1&spec_comps=9&player_id1=e342ad68&p1yrfrom=2024-2025&player_id2=1f44ac21&p2yrfrom=2024-2025&player_id3=8e92be30&p3yrfrom=2024-2025&player_id4=dc7f8a28&p4yrfrom=2024-2025&player_id5=4e9a0555&p5yrfrom=2024-2025&player_id6=6afaebf2&p6yrfrom=2024-2025&player_id7=dc62b55d&p7yrfrom=2024-2025\"\n",
    "    \n",
    "    # Fetch the page content\n",
    "    response = requests.get(epl_scores_url)\n",
    "    \n",
    "    # Parse the HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Locate the scorers table using its ID\n",
    "    scorers = soup.find('table', {'id': 'standard_stats'})\n",
    "    \n",
    "    # Extract only the last row of headers (normal header row)\n",
    "    headers = [th.text.strip() for th in scorers.find('thead').find_all('tr')[-1].find_all('th')]\n",
    "    \n",
    "    # Extract table rows\n",
    "    rows = scorers.find('tbody').find_all('tr')\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        # Extract all cells in the row\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        data.append([cell.text.strip() for cell in cells])\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    epl_scorers = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    \n",
    "    # Get the column names as a list\n",
    "    columns = list(epl_scorers.columns)\n",
    "    \n",
    "    # Replace only the first occurrence of 'Gls' with 'Goals'\n",
    "    for i, col in enumerate(columns):\n",
    "        if col == 'Gls':\n",
    "            columns[i] = 'Goals'\n",
    "            break  \n",
    "    \n",
    "    # Assign the updated column names back to the DataFrame\n",
    "    epl_scorers.columns = columns\n",
    "    \n",
    "    # Preview the DataFrame\n",
    "    epl_scorers.head()\n",
    "    \n",
    "    epl_scorers_sql = epl_scorers[['Player','Span','Nation','Pos','Squad','MP','Goals']]\n",
    "    \n",
    "    epl_scorers_sql['league']='Epl'\n",
    "    \n",
    "    epl_scorers_sql.head()\n",
    "    \n",
    "\n",
    "    # Handle NaN values\n",
    "    epl_scorers_sql = epl_scorers_sql.where(pd.notnull(epl_scorers_sql), None)\n",
    "    \n",
    "    # Connect to SQL Server\n",
    "    server = 'Max\\SQLEXPRESS01'\n",
    "    database = 'Football'\n",
    "    conn = pyodbc.connect(f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    \n",
    "    # Clear the existing table\n",
    "    try:\n",
    "        truncate_query = \"TRUNCATE TABLE EPL_TopScorers\"  # Use TRUNCATE for faster clearing if no foreign key constraints\n",
    "        cursor.execute(truncate_query)\n",
    "        print(\"Table cleared successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error clearing the table: {e}\")\n",
    "    \n",
    "    # Insert new data\n",
    "    try:\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO  EPL_TopScorers(Player,Span,Nation,Pos,Squad,MP,Goals,league)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "        for index, row in epl_scorers_sql.iterrows():\n",
    "            cursor.execute(insert_query, row['Player'], row['Span'], row['Nation'], row['Pos'], row['Squad'], row['MP'], row['Goals'], row['league'])\n",
    "    \n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "        print(\"Table updated successfully with the latest epl Top Scoreres.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting new data: {e}\")\n",
    "    \n",
    "    # Close the connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Lalig stats \n",
    "\n",
    "    \n",
    "    # LALIGA STANDINGS\n",
    "    \n",
    "    laliga_url = \"https://fbref.com/en/comps/12/La-Liga-Stats\"\n",
    "    \n",
    "    # Fetch the data\n",
    "    response = requests.get(laliga_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Locate the standings table\n",
    "    standings_table = soup.select('table.stats_table')[0]\n",
    "    \n",
    "    # Extract headers\n",
    "    headers = [th.text for th in standings_table.select('thead th')]\n",
    "    \n",
    "    # Extract rows\n",
    "    rows = standings_table.select('tbody tr')\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        row_data = [td.text.strip() for td in row.select('td')]\n",
    "        # Include rank from <th> (not inside <td>)\n",
    "        rank = row.select_one('th').text.strip()\n",
    "        data.append([rank] + row_data)\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    laliga_standings = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    laliga_standings.head()\n",
    "    \n",
    "    \n",
    "    # Filter necessary columns\n",
    "    laliga_standings_sql = laliga_standings[['Rk', 'Squad', 'MP', 'W', 'D', 'L', 'Pts', 'Last 5', 'Top Team Scorer']]\n",
    "    \n",
    "    laliga_standings_sql['league'] = 'Laliga'\n",
    "    \n",
    "    laliga_standings_sql.rename(\n",
    "        columns= {\n",
    "            'Last 5': 'Last_5',\n",
    "            'Top Team Scorer' : 'Top_Team_Scorer'\n",
    "        },inplace = True\n",
    "    )\n",
    "    \n",
    "    laliga_standings_sql.head()\n",
    "\n",
    "    \n",
    "    # Handle NaN values\n",
    "    laliga_standings_sql = laliga_standings_sql.where(pd.notnull(laliga_standings_sql), None)\n",
    "    \n",
    "    # Connect to SQL Server\n",
    "    server = 'Max\\SQLEXPRESS01'\n",
    "    database = 'Football'\n",
    "    conn = pyodbc.connect(f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Upsert logic using MERGE\n",
    "    for index, row in laliga_standings_sql.iterrows():\n",
    "        merge_query = \"\"\"\n",
    "        MERGE laliga_Standings AS target\n",
    "        USING (VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)) AS source (Rk, Squad, MP, W, D, L, Pts, Last_5, Top_Team_Scorer)\n",
    "        ON target.Squad = source.Squad\n",
    "        WHEN MATCHED THEN \n",
    "            UPDATE SET \n",
    "                Rk = source.Rk, \n",
    "                MP = source.MP, \n",
    "                W = source.W, \n",
    "                D = source.D, \n",
    "                L = source.L, \n",
    "                Pts = source.Pts, \n",
    "                Last_5 = source.Last_5, \n",
    "                Top_Team_Scorer = source.Top_Team_Scorer\n",
    "        WHEN NOT MATCHED THEN \n",
    "            INSERT (Rk, Squad, MP, W, D, L, Pts, Last_5, Top_Team_Scorer)\n",
    "            VALUES (source.Rk, source.Squad, source.MP, source.W, source.D, source.L, source.Pts, source.Last_5, source.Top_Team_Scorer);\n",
    "        \"\"\"\n",
    "        cursor.execute(merge_query, row['Rk'], row['Squad'], row['MP'], row['W'], row['D'], row['L'], row['Pts'], row['Last_5'], row['Top_Team_Scorer'])\n",
    "    \n",
    "    # Commit the transaction and close the connection\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"Table updated successfully with the latest laliga standings.\")\n",
    "\n",
    "\n",
    "# LALIGA FIXTURES\n",
    "    \n",
    "    lal_fixture_url = \"https://fbref.com/en/comps/12/schedule/La-Liga-Scores-and-Fixtures\"\n",
    "    \n",
    "    response = requests.get(lal_fixture_url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    lal_sched_2024_2025 = soup.find('table',{'id': 'sched_2024-2025_12_1'})\n",
    "    \n",
    "    headers = [th.text for th in lal_sched_2024_2025.find('thead').find_all('th')]\n",
    "    \n",
    "    rows = lal_sched_2024_2025.find('tbody').find_all('tr')\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        # Extract all cells in each row\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        data.append([cell.text.strip() for cell in cells])\n",
    "    \n",
    "    lal_schd = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    lal_schd.head()\n",
    "    \n",
    "    lal_schd_sql = lal_schd[['Wk','Day','Date','Time','Home','Score','Away','Venue']]\n",
    "    \n",
    "    lal_schd_sql['league'] = 'Laliga'\n",
    "    \n",
    "    lal_schd_sql.head()\n",
    "\n",
    "       \n",
    "    # Handle NaN values\n",
    "    lal_schd_sql = lal_schd_sql.where(pd.notnull(lal_schd_sql), None)\n",
    "    \n",
    "    # Connect to SQL Server\n",
    "    server = 'Max\\SQLEXPRESS01'\n",
    "    database = 'Football'\n",
    "    conn = pyodbc.connect(f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Upsert logic using MERGE\n",
    "    for index, row in lal_schd_sql.iterrows():\n",
    "        merge_query = \"\"\"\n",
    "        MERGE LaLiga_Schedule AS target\n",
    "        USING (VALUES (?, ?, ?, ?, ?, ?, ?,?)) AS source (Wk, Day, Date, Time, Home, Score, Away,Venue)\n",
    "        ON target.Wk = source.Wk and target.Date = source.Date and target.Home = source.Home and target.Away = source.Away\n",
    "        WHEN MATCHED THEN \n",
    "            UPDATE SET \n",
    "                Wk = source.Wk, \n",
    "                Day = source.Day, \n",
    "                Date = source.Date, \n",
    "                Time = source.Time, \n",
    "                Home = source.Home, \n",
    "                Score = source.Score, \n",
    "                Away = source.Away,\n",
    "                Venue = source.Venue;\n",
    "       \n",
    "        \"\"\"\n",
    "        cursor.execute(merge_query, row['Wk'], row['Day'], row['Date'], row['Time'], row['Home'], row['Score'], row['Away'],row['Venue'])\n",
    "    \n",
    "    # Commit the transaction and close the connection\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"Table updated successfully with the latest laliga schd.\")\n",
    "\n",
    "\n",
    "    # LALIGA SCORERS\n",
    "    \n",
    "    lal_scores_url = \"https://fbref.com/en/stathead/player_comparison.cgi?request=1&sum=0&comp_type=spec&dom_lg=1&spec_comps=12&player_id1=8d78e732&p1yrfrom=2024-2025&player_id2=3423f250&p2yrfrom=2024-2025&player_id3=42fd9c7f&p3yrfrom=2024-2025&player_id4=8f3565b3&p4yrfrom=2024-2025&player_id5=0c61c77c&p5yrfrom=2024-2025&player_id6=7111d552&p6yrfrom=2024-2025&player_id7=819aa8e7&p7yrfrom=2024-2025\"\n",
    "    \n",
    "    # Fetch the page content\n",
    "    response = requests.get(lal_scores_url)\n",
    "    \n",
    "    # Parse the HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Locate the scorers table using its ID\n",
    "    scorers = soup.find('table', {'id': 'standard_stats'})\n",
    "    \n",
    "    # Extract only the last row of headers (normal header row)\n",
    "    headers = [th.text.strip() for th in scorers.find('thead').find_all('tr')[-1].find_all('th')]\n",
    "    \n",
    "    # Extract table rows\n",
    "    rows = scorers.find('tbody').find_all('tr')\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        # Extract all cells in the row\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        data.append([cell.text.strip() for cell in cells])\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    lal_scorers = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    \n",
    "    columns = list(lal_scorers.columns)\n",
    "    \n",
    "    # Replace only the first occurrence of 'Gls' with 'Goals'\n",
    "    for i, col in enumerate(columns):\n",
    "        if col == 'Gls':\n",
    "            columns[i] = 'Goals'\n",
    "            break  # Exit the loop after replacing the first occurrence\n",
    "    \n",
    "    # Assign the updated column names back to the DataFrame\n",
    "    lal_scorers.columns = columns\n",
    "    \n",
    "    lal_scorers_sql = lal_scorers[['Player','Span','Nation','Pos','Squad','MP','Goals']]\n",
    "    \n",
    "    lal_scorers_sql['league']='Laliga'\n",
    "    \n",
    "    lal_scorers_sql.head()\n",
    "    \n",
    "\n",
    "    # Handle NaN values\n",
    "    lal_scorers_sql = lal_scorers_sql.where(pd.notnull(lal_scorers_sql), None)\n",
    "    \n",
    "    # Connect to SQL Server\n",
    "    server = 'Max\\SQLEXPRESS01'\n",
    "    database = 'Football'\n",
    "    conn = pyodbc.connect(f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    \n",
    "    # Clear the existing table\n",
    "    try:\n",
    "        truncate_query = \"TRUNCATE TABLE laliga_TopScorers\"  # Use TRUNCATE for faster clearing if no foreign key constraints\n",
    "        cursor.execute(truncate_query)\n",
    "        print(\"Table cleared successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error clearing the table: {e}\")\n",
    "    \n",
    "    # Insert new data\n",
    "    try:\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO  laliga_TopScorers (Player,Span,Nation,Pos,Squad,MP,Goals,league)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "        for index, row in lal_scorers_sql.iterrows():\n",
    "            cursor.execute(insert_query, row['Player'], row['Span'], row['Nation'], row['Pos'], row['Squad'], row['MP'], row['Goals'], row['league'])\n",
    "    \n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "        print(\"Table updated successfully with the latest laliga top scorers.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting new data: {e}\")\n",
    "    \n",
    "    # Close the connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "#BUNDESLIGA\n",
    "    \n",
    "    # BUNDESLIGA STANDING\n",
    "    \n",
    "    bundesliga_url = \"https://fbref.com/en/comps/20/Bundesliga-Stats\"\n",
    "    \n",
    "    # Fetch the data\n",
    "    response = requests.get(bundesliga_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Locate the standings table\n",
    "    standings_table = soup.select('table.stats_table')[0]\n",
    "    \n",
    "    # Extract headers\n",
    "    headers = [th.text for th in standings_table.select('thead th')]\n",
    "    \n",
    "    # Extract rows\n",
    "    \n",
    "    rows = standings_table.select('tbody tr')\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        row_data = [td.text.strip() for td in row.select('td')]\n",
    "        # Include rank from <th> (not inside <td>)\n",
    "        rank = row.select_one('th').text.strip()\n",
    "        data.append([rank] + row_data)\n",
    "    \n",
    "    \n",
    "    # Create a DataFrame\n",
    "    bundesliga_standings = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    bundesliga_standings.head()\n",
    "    \n",
    "    \n",
    "    bundesliga_standings_sql = bundesliga_standings[['Rk', 'Squad', 'MP', 'W', 'D', 'L', 'Pts', 'Last 5', 'Top Team Scorer']]\n",
    "    \n",
    "    bundesliga_standings['league'] = 'Laliga'\n",
    "    \n",
    "    bundesliga_standings_sql.rename(\n",
    "        columns= {\n",
    "            'Last 5': 'Last_5',\n",
    "            'Top Team Scorer' : 'Top_Team_Scorer'\n",
    "        },inplace = True\n",
    "    )\n",
    "    \n",
    "    bundesliga_standings_sql.head()\n",
    "\n",
    "\n",
    "    # Handle NaN values\n",
    "    bundesliga_standings_sql = bundesliga_standings_sql.where(pd.notnull(bundesliga_standings_sql), None)\n",
    "    \n",
    "    # Connect to SQL Server\n",
    "    server = 'Max\\SQLEXPRESS01'\n",
    "    database = 'Football'\n",
    "    conn = pyodbc.connect(f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Upsert logic using MERGE\n",
    "    for index, row in bundesliga_standings_sql.iterrows():\n",
    "        merge_query = \"\"\"\n",
    "        MERGE bun_Standings AS target\n",
    "        USING (VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)) AS source (Rk, Squad, MP, W, D, L, Pts, Last_5, Top_Team_Scorer)\n",
    "        ON target.Squad = source.Squad\n",
    "        WHEN MATCHED THEN \n",
    "            UPDATE SET \n",
    "                Rk = source.Rk, \n",
    "                MP = source.MP, \n",
    "                W = source.W, \n",
    "                D = source.D, \n",
    "                L = source.L, \n",
    "                Pts = source.Pts, \n",
    "                Last_5 = source.Last_5, \n",
    "                Top_Team_Scorer = source.Top_Team_Scorer\n",
    "        WHEN NOT MATCHED THEN \n",
    "            INSERT (Rk, Squad, MP, W, D, L, Pts, Last_5, Top_Team_Scorer)\n",
    "            VALUES (source.Rk, source.Squad, source.MP, source.W, source.D, source.L, source.Pts, source.Last_5, source.Top_Team_Scorer);\n",
    "        \"\"\"\n",
    "        cursor.execute(merge_query, row['Rk'], row['Squad'], row['MP'], row['W'], row['D'], row['L'], row['Pts'], row['Last_5'], row['Top_Team_Scorer'])\n",
    "    \n",
    "    # Commit the transaction and close the connection\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"Table updated successfully with the latest bundesliga standings.\")\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "    # BUNDESLIGA FIXTURE\n",
    "    \n",
    "    Bundesliga_fixture_url = \"https://fbref.com/en/comps/20/schedule/Bundesliga-Scores-and-Fixtures\"\n",
    "    \n",
    "    response = requests.get(Bundesliga_fixture_url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    bun_sched_2024_2025 = soup.find('table',{'id': 'sched_2024-2025_20_1'})\n",
    "    \n",
    "    headers = [th.text for th in bun_sched_2024_2025.find('thead').find_all('th')]\n",
    "    \n",
    "    rows = bun_sched_2024_2025.find('tbody').find_all('tr')\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        # Extract all cells in each row\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        data.append([cell.text.strip() for cell in cells])\n",
    "    \n",
    "    bun_schd = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    bun_schd.head() \n",
    "    \n",
    "    bun_schd_sql = bun_schd[['Wk','Day','Date','Time','Home','Score','Away','Venue']]\n",
    "    \n",
    "    bun_schd_sql['league'] = 'bundesliga'\n",
    "    \n",
    "    bun_schd_sql.head()\n",
    "\n",
    "   \n",
    "    # Handle NaN values\n",
    "    bun_schd_sql = bun_schd_sql.where(pd.notnull(bun_schd_sql), None)\n",
    "    \n",
    "    # Connect to SQL Server\n",
    "    server = 'Max\\SQLEXPRESS01'\n",
    "    database = 'Football'\n",
    "    conn = pyodbc.connect(f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Upsert logic using MERGE\n",
    "    for index, row in bun_schd_sql.iterrows():\n",
    "        merge_query = \"\"\"\n",
    "        MERGE Bundesliga_Schedule AS target\n",
    "        USING (VALUES (?, ?, ?, ?, ?, ?, ?,?)) AS source (Wk, Day, Date, Time, Home, Score, Away,Venue)\n",
    "        ON target.Wk = source.Wk and target.Date = source.Date and target.Home = source.Home and target.Away = source.Away\n",
    "        WHEN MATCHED THEN \n",
    "            UPDATE SET \n",
    "                Wk = source.Wk, \n",
    "                Day = source.Day, \n",
    "                Date = source.Date, \n",
    "                Time = source.Time, \n",
    "                Home = source.Home, \n",
    "                Score = source.Score, \n",
    "                Away = source.Away,\n",
    "                Venue = source.Venue;\n",
    "       \n",
    "        \"\"\"\n",
    "        cursor.execute(merge_query, row['Wk'], row['Day'], row['Date'], row['Time'], row['Home'], row['Score'], row['Away'],row['Venue'])\n",
    "    \n",
    "    # Commit the transaction and close the connection\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"Table updated successfully with the latest bundesliga schd.\")\n",
    "\n",
    "    \n",
    "    # BUNDESLIGA SCORERES\n",
    "    \n",
    "    bun_scores_url = \"https://fbref.com/en/stathead/player_comparison.cgi?request=1&sum=0&comp_type=spec&dom_lg=1&spec_comps=20&player_id1=21a66f6a&p1yrfrom=2024-2025&player_id2=0e0102eb&p2yrfrom=2024-2025&player_id3=05f99a4a&p3yrfrom=2024-2025&player_id4=258a6f4d&p4yrfrom=2024-2025&player_id5=2c0558b8&p5yrfrom=2024-2025&player_id6=5d4f7d61&p6yrfrom=2024-2025&player_id7=1fffae99&p7yrfrom=2024-2025\"\n",
    "    \n",
    "    # Fetch the page content\n",
    "    response = requests.get(bun_scores_url)\n",
    "    \n",
    "    # Parse the HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Locate the scorers table using its ID\n",
    "    scorers = soup.find('table', {'id': 'standard_stats'})\n",
    "    \n",
    "    # Extract only the last row of headers (normal header row)\n",
    "    headers = [th.text.strip() for th in scorers.find('thead').find_all('tr')[-1].find_all('th')]\n",
    "    \n",
    "    # Extract table rows\n",
    "    rows = scorers.find('tbody').find_all('tr')\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        # Extract all cells in the row\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        data.append([cell.text.strip() for cell in cells])\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    bun_scorers = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    columns = list(bun_scorers.columns)\n",
    "    \n",
    "    # Replace only the first occurrence of 'Gls' with 'Goals'\n",
    "    for i, col in enumerate(columns):\n",
    "        if col == 'Gls':\n",
    "            columns[i] = 'Goals'\n",
    "            break  # Exit the loop after replacing the first occurrence\n",
    "    \n",
    "    # Assign the updated column names back to the DataFrame\n",
    "    bun_scorers.columns = columns\n",
    "    # Preview the DataFrame\n",
    "    bun_scorers.head()\n",
    "    \n",
    "  \n",
    "    # Assign the updated column names back to the DataFrame\n",
    "    bun_scorers.columns = columns\n",
    "    \n",
    "    bun_scorers_sql = bun_scorers[['Player','Span','Nation','Pos','Squad','MP','Goals']]\n",
    "    \n",
    "    bun_scorers_sql['league']='bundesliga'\n",
    "    \n",
    "    bun_scorers_sql.head()\n",
    "\n",
    "    # Handle NaN values\n",
    "    bun_scorers_sql = bun_scorers_sql.where(pd.notnull(bun_scorers_sql), None)\n",
    "    \n",
    "    # Connect to SQL Server\n",
    "    server = 'Max\\SQLEXPRESS01'\n",
    "    database = 'Football'\n",
    "    conn = pyodbc.connect(f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    \n",
    "    # Clear the existing table\n",
    "    try:\n",
    "        truncate_query = \"TRUNCATE TABLE bundesliga_TopScorers\"  # Use TRUNCATE for faster clearing if no foreign key constraints\n",
    "        cursor.execute(truncate_query)\n",
    "        print(\"Table cleared successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error clearing the table: {e}\")\n",
    "    \n",
    "    # Insert new data\n",
    "    try:\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO  bundesliga_TopScorers (Player,Span,Nation,Pos,Squad,MP,Goals,league)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "        for index, row in bun_scorers_sql.iterrows():\n",
    "            cursor.execute(insert_query, row['Player'], row['Span'], row['Nation'], row['Pos'], row['Squad'], row['MP'], row['Goals'], row['league'])\n",
    "    \n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "        print(\"Table updated successfully with the latest bundesliga top scorers.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting new data: {e}\")\n",
    "\n",
    "\n",
    "# #SERIA STATS\n",
    "    \n",
    "    # SERIA STANDINGS\n",
    "    \n",
    "    serie_a_url = \"https://fbref.com/en/comps/11/Serie-A-Stats\"\n",
    "    \n",
    "    # Fetch the data\n",
    "    response = requests.get(serie_a_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Locate the standings table\n",
    "    standings_table = soup.select('table.stats_table')[0]\n",
    "    \n",
    "    # Extract headers\n",
    "    headers = [th.text for th in standings_table.select('thead th')]\n",
    "    \n",
    "    # Extract rows\n",
    "    rows = standings_table.select('tbody tr')\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        row_data = [td.text.strip() for td in row.select('td')]\n",
    "        # Include rank from <th> (not inside <td>)\n",
    "        rank = row.select_one('th').text.strip()\n",
    "        data.append([rank] + row_data)\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    seria_standings = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    \n",
    "    seria_standings.head()\n",
    "    \n",
    "    seria_standings_sql = seria_standings[['Rk', 'Squad', 'MP', 'W', 'D', 'L', 'Pts', 'Last 5', 'Top Team Scorer']]\n",
    "    \n",
    "    seria_standings_sql['league'] = 'Laliga'\n",
    "    \n",
    "    seria_standings_sql.rename(\n",
    "        columns= {\n",
    "            'Last 5': 'Last_5',\n",
    "            'Top Team Scorer' : 'Top_Team_Scorer'\n",
    "        },inplace = True\n",
    "    )\n",
    "    \n",
    "    seria_standings_sql.head()\n",
    "\n",
    "    \n",
    "    # Handle NaN values\n",
    "    seria_standings_sql = seria_standings_sql.where(pd.notnull(seria_standings_sql), None)\n",
    "    \n",
    "    # Connect to SQL Server\n",
    "    server = 'Max\\SQLEXPRESS01'\n",
    "    database = 'Football'\n",
    "    conn = pyodbc.connect(f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Upsert logic using MERGE\n",
    "    for index, row in seria_standings_sql.iterrows():\n",
    "        merge_query = \"\"\"\n",
    "        MERGE seria_Standings AS target\n",
    "        USING (VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)) AS source (Rk, Squad, MP, W, D, L, Pts, Last_5, Top_Team_Scorer)\n",
    "        ON target.Squad = source.Squad\n",
    "        WHEN MATCHED THEN \n",
    "            UPDATE SET \n",
    "                Rk = source.Rk, \n",
    "                MP = source.MP, \n",
    "                W = source.W, \n",
    "                D = source.D, \n",
    "                L = source.L, \n",
    "                Pts = source.Pts, \n",
    "                Last_5 = source.Last_5, \n",
    "                Top_Team_Scorer = source.Top_Team_Scorer\n",
    "        WHEN NOT MATCHED THEN \n",
    "            INSERT (Rk, Squad, MP, W, D, L, Pts, Last_5, Top_Team_Scorer)\n",
    "            VALUES (source.Rk, source.Squad, source.MP, source.W, source.D, source.L, source.Pts, source.Last_5, source.Top_Team_Scorer);\n",
    "        \"\"\"\n",
    "        cursor.execute(merge_query, row['Rk'], row['Squad'], row['MP'], row['W'], row['D'], row['L'], row['Pts'], row['Last_5'], row['Top_Team_Scorer'])\n",
    "    \n",
    "    # Commit the transaction and close the connection\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"Table updated successfully with the latest seria standings.\")\n",
    "\n",
    "\n",
    "\n",
    "    # SERIA FIXTURE\n",
    "    \n",
    "    seria_fixture_url = \"https://fbref.com/en/comps/11/schedule/Serie-A-Scores-and-Fixtures\"\n",
    "    \n",
    "    response = requests.get(seria_fixture_url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    seria_sched_2024_2025 = soup.find('table',{'id': 'sched_2024-2025_11_1'})\n",
    "    \n",
    "    headers = [th.text for th in seria_sched_2024_2025.find('thead').find_all('th')]\n",
    "    \n",
    "    rows = seria_sched_2024_2025.find('tbody').find_all('tr')\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        # Extract all cells in each row\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        data.append([cell.text.strip() for cell in cells])\n",
    "    \n",
    "    seria_schd = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    seria_schd.head()\n",
    "    \n",
    "    seria_schd_sql = seria_schd[['Wk','Day','Date','Time','Home','Score','Away','Venue']]\n",
    "    \n",
    "    seria_schd_sql['league'] = 'Seria'\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "    seria_schd_sql = seria_schd_sql.where(pd.notnull(seria_schd_sql), None)\n",
    "    \n",
    "    # Connect to SQL Server\n",
    "    server = 'Max\\SQLEXPRESS01'\n",
    "    database = 'Football'\n",
    "    conn = pyodbc.connect(f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Upsert logic using MERGE\n",
    "    for index, row in seria_schd_sql.iterrows():\n",
    "        merge_query = \"\"\"\n",
    "        MERGE Seria_Schedule AS target\n",
    "        USING (VALUES (?, ?, ?, ?, ?, ?, ?,?)) AS source (Wk, Day, Date, Time, Home, Score, Away,Venue)\n",
    "        ON target.Wk = source.Wk and target.Date = source.Date and target.Home = source.Home and target.Away = source.Away\n",
    "        WHEN MATCHED THEN \n",
    "            UPDATE SET \n",
    "                Wk = source.Wk, \n",
    "                Day = source.Day, \n",
    "                Date = source.Date, \n",
    "                Time = source.Time, \n",
    "                Home = source.Home, \n",
    "                Score = source.Score, \n",
    "                Away = source.Away,\n",
    "                Venue = source.Venue;\n",
    "       \n",
    "        \"\"\"\n",
    "        cursor.execute(merge_query, row['Wk'], row['Day'], row['Date'], row['Time'], row['Home'], row['Score'], row['Away'],row['Venue'])\n",
    "    \n",
    "    # Commit the transaction and close the connection\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(\"Table updated successfully with the latest seria schd.\")\n",
    "\n",
    " \n",
    "    # SERIA SCORERS\n",
    "    \n",
    "    seria_scores_url = \"https://fbref.com/en/stathead/player_comparison.cgi?request=1&sum=0&comp_type=spec&dom_lg=1&spec_comps=11&player_id1=6f8cd6d0&p1yrfrom=2024-2025&player_id2=e7695e6c&p2yrfrom=2024-2025&player_id3=3d50bcdb&p3yrfrom=2024-2025&player_id4=7c104bb7&p4yrfrom=2024-2025&player_id5=79443529&p5yrfrom=2024-2025&player_id6=83c06f3a&p6yrfrom=2024-2025&player_id7=da76bab4&p7yrfrom=2024-2025\"\n",
    "    \n",
    "    # Fetch the page content\n",
    "    response = requests.get(seria_scores_url)\n",
    "    \n",
    "    # Parse the HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Locate the scorers table using its ID\n",
    "    scorers = soup.find('table', {'id': 'standard_stats'})\n",
    "    \n",
    "    # Extract only the last row of headers (normal header row)\n",
    "    headers = [th.text.strip() for th in scorers.find('thead').find_all('tr')[-1].find_all('th')]\n",
    "    \n",
    "    # Extract table rows\n",
    "    rows = scorers.find('tbody').find_all('tr')\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        # Extract all cells in the row\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        data.append([cell.text.strip() for cell in cells])\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    seria_scorers = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    columns = list(seria_scorers.columns)\n",
    "    \n",
    "    # Replace only the first occurrence of 'Gls' with 'Goals'\n",
    "    for i, col in enumerate(columns):\n",
    "        if col == 'Gls':\n",
    "            columns[i] = 'Goals'\n",
    "            break  # Exit the loop after replacing the first occurrence\n",
    "    \n",
    "    # Assign the updated column names back to the DataFrame\n",
    "    seria_scorers.columns = columns\n",
    "    \n",
    "    # Preview the DataFrame\n",
    "    seria_scorers.head()\n",
    "    \n",
    "    # Assign the updated column names back to the DataFrame\n",
    "    seria_scorers.columns = columns\n",
    "    \n",
    "    seria_scorers_sql = seria_scorers[['Player','Span','Nation','Pos','Squad','MP','Goals']]\n",
    "    \n",
    "    seria_scorers_sql['league']='seria'\n",
    "    \n",
    "    seria_scorers_sql.head()\n",
    "\n",
    "\n",
    "    # Handle NaN values\n",
    "    seria_scorers_sql = seria_scorers_sql.where(pd.notnull(seria_scorers_sql), None)\n",
    "    \n",
    "    # Connect to SQL Server\n",
    "    server = 'Max\\SQLEXPRESS01'\n",
    "    database = 'Football'\n",
    "    conn = pyodbc.connect(f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};Trusted_Connection=yes;\")\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    \n",
    "    # Clear the existing table\n",
    "    try:\n",
    "        truncate_query = \"TRUNCATE TABLE seria_TopScorers\"  # Use TRUNCATE for faster clearing if no foreign key constraints\n",
    "        cursor.execute(truncate_query)\n",
    "        print(\"Table cleared successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error clearing the table: {e}\")\n",
    "    \n",
    "    # Insert new data\n",
    "    try:\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO  seria_TopScorers (Player,Span,Nation,Pos,Squad,MP,Goals,league)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "        for index, row in seria_scorers_sql.iterrows():\n",
    "            cursor.execute(insert_query, row['Player'], row['Span'], row['Nation'], row['Pos'], row['Squad'], row['MP'], row['Goals'], row['league'])\n",
    "    \n",
    "        # Commit the transaction\n",
    "        conn.commit()\n",
    "        print(\"Table updated successfully with the latest seria Top Scorers.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting new data: {e}\")\n",
    "    \n",
    "    # Close the connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "while(True):\n",
    "    update_football_tables()\n",
    "    time.sleep(60)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c304f398-7a42-4f42-92a5-bb6f6152f6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2836c492-ea29-471f-83a4-75d5719c620a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710590ac-3c19-44e4-bc46-9c3b14b28c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe462b-7fb4-4439-8536-b81abf25d810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79409daf-7bc4-4d21-8d04-6c105486d523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0422bc-3458-415f-8a72-c5c89f79d298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454185d9-d845-4a4d-b86b-c3ad518ddcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf2527-3085-48f9-9ce2-c7d884ac5f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985361a9-4062-4ff9-8d9c-343a35522f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87692202-8cfe-4ee2-aa45-d8e87ecea868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf66eecc-0c5d-491f-8dce-b59ec53908e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a9343-6e2a-49bf-8c23-81cb5c8c57b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b078e2-b521-4328-b37f-cf456d070eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa53ba-61b9-48db-bf39-b22428bc77ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c963f-5c1f-4a49-b09c-5cb9592d5dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aba2001-ba64-45f5-a1f3-93ab0edf2a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762029cd-f3e6-424d-8d30-c913b3906852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
